---
layout: post
title:  "Day 7 – It's Possible"
date:   2025-06-13 10:00:00 +0530
categories: jekyll update
---
## Friday, June 13 — Is it Possible ?


#### 🛠️ A few weeks ago, even the thought of cracking an interview like this would have been unimaginable. But now my mindset is changing.

---

### 🚀 Job description:
- Strong proficiency in SQL and at least one programming language (e.g., Python, Java).
- Experience with data pipeline tools and frameworks 
- Experience with cloud-based data warehousing solutions (Snowflake).
- Experience with AWS Kinesis, SNS, SQS
- Excellent problem-solving and analytical skills.

### 🚀 You will:
- Design, develop, and maintain robust and scalable data pipelines that ingest, transform, and load data from various sources into data warehouse.
- Collaborate with business stakeholders to understand data requirements and translate them into technical solutions.
- Implement data quality checks and monitoring to ensure data accuracy and integrity.
- Optimize data pipelines for performance and efficiency.
- Troubleshoot and resolve data pipeline issues.
- Stay up-to-date with emerging technologies and trends in data engineering.

### 🚀 Key Skills:
- Data pipeline architecture
- Data warehousing
- ETL (Extract, Transform, Load)
- Data modeling
- SQL
- Python or Java or Go
- Cloud computing
- Business intelligence

### 🛠️ Though I started off very confident, this YT video got me really scared:

- [All Data Engineering Interview Rounds Explained!
](https://www.youtube.com/watch?v=jdrwZfeTd-o)

### 🔍🚀🛠️ Wrapped up a mini ETL project using **Apache Airflow**, **NASA’s APOD API**, and **PostgreSQL**.

---

### 🚀 Code Summary in 4 Lines

1. Used `SimpleHttpOperator` to fetch the Astronomy Picture of the Day from NASA’s API.  
2. Transformed the response using a Python task to keep only the relevant fields.  
3. Created a Postgres table (if not exists) via `PostgresHook`.  
4. Inserted the cleaned data into the table using a parameterized SQL query.

---

### 🔍 Airflow Concepts Applied

- **DAG**: The pipeline is defined as a DAG scheduled to run daily (`@daily`), ensuring idempotent ETL behavior.
- **Hooks**: `PostgresHook` allows executing SQL queries securely and cleanly from Python tasks.
- **Operators**: `SimpleHttpOperator` handled the API request; `@task` decorators handled transformation and loading.
- **Connections**: Airflow’s Connection UI is used to store both the API key (`nasa_api`) and DB credentials (`my_postgres_connection`), keeping the code clean and secure.
- **Task Dependencies**: DAG flow enforces the order: create table → extract → transform → load.


### Studied/Researched

- [Data Storage Comparison + SQL](https://chatgpt.com/share/684be575-1bd0-800e-8b1d-5ee6393a2c54)
- [Snowflake Optimization Strategies](https://chatgpt.com/share/684e8451-43a4-800e-a523-57c6db73ba59)
- [Snowflake vs Redshift](https://chatgpt.com/share/684e8538-bb7c-800e-9d9a-729c66c9c151)
- [Airflow NASA ETL](https://chatgpt.com/share/684e849b-687c-800e-bcbe-434bea19f4a5)
- [Bigeye vs Airflow Comparison](https://chatgpt.com/share/684e8511-8f34-800e-8222-3bc5a4890e61)
- [Improving Transformations with DBT and Airflow](https://chatgpt.com/share/684e8618-c62c-800e-9aaf-dfabab30085b)